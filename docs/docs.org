* Introduction
** TODO Zsyntax

   The main aim of [paper 2015] is to suggest that logic play a significant role
   in the toolbox of molecular biology, particularly for systematizing and
   formalizing certain molecular mechanisms characterized by state transitions,
   that can therefore sensibly be described as ``inferential''.  The article
   shows how biochemical pathways, i.e., transitions from a molecular aggregate
   to another molecular aggregate, can be viewed as deductive processes.

   As already pointed out by Girard, state transition systems, like the ones
   considered in biochemistry, can be effectively described by linear logic.  In
   essence, by representing chemical equations as axioms, the notion of linear
   consequence corresponds to the notion of accessible state from an initial
   one. Moreover, the complete description of the intermediate state transitions
   can easily be extracted from the deduction itself.
   In [paper 2015], the authors follow Girard's suggestion and outline a
   correspondence between linear deductions and (bio)chemical reactions,
   observing, however, that a modification is needed for representing the kind
   of state transitions that are typical of biomolecular models.

   In order to represent the transition from a molecular aggregate to another
   molecular aggregate as a deductive process, a logical system called Zsyntax
   is used. The formal language of Zsyntax is inductively defined from a set of
   atomic formulas (defined in [2015 paper] as the bonding language
   $\bioformulas$), combined with some linear logic-inspired operators:

   - aggregative Z-conjunction $\otimes$ (which corresponds to multiplicative
     conjunction)
   - Z-conditional $\rightarrow$ (which corresponds to linear implication)
   - selective Z-conjunction $\wedge$ (which corresponds to additive
     conjunction)
   - Z-disjunction $\vee$ (which corresponds to additive disjunction)
   - a unit formula $\top$ (which corresponds to the multiplicative conjunction
     unit)

   Given the meaning of the Zsyntax operators, the formulas in the resulting
   language correspond to suitable types of molecular aggregates. A proof of
   $\Gamma \models \Delta$ then represents the biochemical pathway from the
   aggregate $\Gamma$ to the aggregate $\Delta$, where $\models$ is defined in
   [2015 paper] as the analogous of a logical consequence relation.
   Proofs are built according to a logical calculus formulated in natural
   deduction style.

   The main difference between Zsyntax and linear logic lies in the
   non-monotonicity of the logical consequence relation (and correspondingly of
   the linear implication operator, here called Z-conditional).
   To clarify, monotonicity of implication means that the validity of a formula
   $A \rightarrow B$ implies the validity of $A \otimes C \rightarrow B \otimes
   C$ for any $C$. This is at odds with the fact that many imiportant state
   transitions of interest in molecular biology are context-sensitive: a
   reaction may not take place depending on the molecular context in which it
   occurs.

   The logical system described by the authors in therefore one that considers
   context-sensitive state transitions: a transition may take place in every
   molecular context that satisfies its control condition, which is a device
   used to describe all contexts that instead inhibit such transition.  Control
   conditions are expressed in the formalism of Zsyntax as \emph{control sets},
   which are intended to be empirically determined (i.e., they result from
   empirical knowledge obtained in the laboratory). The immediate consequence is
   that their content typically changes over time, resulting in a logical system
   that is \emph{open}: theorems may loose their status depending on
   modification of the empirical knowledge.

** TODO Automated deduction

   The objective of this work is to develop an automated theorem prover for (a
   suitable fragment of) the calculus of Zsyntax presented in [paper 2015].
   The accomplishment of such objective required a study of the proof-theoretic
   properties of the logic, a survey of the already existing body of knowledge
   concerning automated deduction for similar logics (and in particular linear
   logic), and the adaptation of procedures from the literature to the specific
   case at hand.

   The logic that has been implemented does not correspond to the full calculus
   presented in [2015 paper], but it is a fragment that has been selected to be
   sufficiently expressive to be useful, sufficiently representative of the
   concept of Zsyntax, and enough compact to be manageable in the short time
   available. In retrospective, this choice proved to be quite appropriate, as
   the sole study and implementation of the aforementioned fragment already took
   several weeks to be completed.

   The development presented here proceeded following a simple yet effective
   approach: a new logic for Zsyntax based on intuitionistic linear logic is
   developed, with proof search in mind.  In particular, this new logic is
   obtained by studying the properties that set Zsyntax apart from linear logic,
   and then adding them to a sequent calculus for linear logic in the form of
   annotations.  This yields a calculus with derivations that are structurally
   identical (modulo annotations) to the ones of linear logic, allowing us to
   (more or less) easily exploit many already existing deduction procedures for
   linear logic. Annotations are then used as a way to tune, or restrict, the
   inference engine to account for the biological constraints.

   [mention cmu thesis]

** TODO Outline

   - Section 2 gives the reasons why plain Zsyntax as given in [2015 paper] is
     not entirely satisfactory when it comes to machine implementation, and then
     proceeds by defining an alternative calculus that is very much related but
     subtly different, and that sets the foundation for the rest of the work, as
     well as the implemented software.
   - Section 3 gives a cut-free sequent calculus translation of the logic of the previous
     section, and establishes soundness and completeness with respect to it. A
     proof of cut admissibility is also given, which is required for the
     completeness proof to go through.
   - Section 4 presents the idea of focusing as a way to improve proof search
     and make it more computationally feasible by removing inessential
     non-determinism. It then adapts the idea to our case, thus developing a
     focused sequent calculus based on the standard one given in the previous
     section.
   - Section 5 presents the idea of forward reasoning as an alternative to the
     more-frequently-used backward reasoning, and draws from the literature to
     show how such alternative approach can be very effective to speed up
     inference for linear logic and related formalisms, like the one used here.
     It then again takes [cmu thesis] as a model to represent the focused
     calculus of the preceding section in a forward-oriented direction.
   - Section 6 explains the benefits of the forward focused calculus developed
     in the previous sections, and gives some details on how to go from the
     definition of the calculus to an implementation of it.
   - Section 7 gives some implementation details on the search procedure, based
     on the forward focused calculus.
   - Section 8 ...
   - Section 9 gives a notion of derivation terms, that is first class objects
     that can be used to represent derivations in the sequent calculus in a
     compact way that is particularly useful in implementations.
   - Section 10 concludes with some remarks on various ways in which the work
     presented here could be extended in the future.

* TODO Focused derivations
** (Introduction)

   The idea, thoroughly developed in [thesis], is "to combine the inverse
   method with the notion of focused derivations. Focused derivations arose in
   the context of logic programming as a way of refining proof search into
   phases. Each phase of the search consisted either of only asynchronous steps
   where non-determinism was immaterial, or of only synchronous steps where key
   choices have to be made. Focusing was thus a way of making "big step"
   derivations: pairs of synchronous and asynchronous steps could be thought of
   as a large derived rule. [...] There derived inference rules constructed by
   focusing can also be used to do forward search in big steps. Thus, the
   intermediate results that are internal to the phases of a focused
   derivations do not have to be explicitly constructed or stored in a sequent
   database. This reduces the size of the sequent database, which is the main
   bottleneck in the inverse method. Because a focusing inverse method prover
   is able to make much larger inferences in much fewer steps, it is able to
   explore the search space much more efficiently."

** Backward focused calculus
** Backward derived rules

   Not really going to use this, but useful to understand and develop the theory
   of focused derived rules. We will adapt all of this to the forward direction
   in the next section.

** Forward derived rules

   Notice: We don't develop a forward focused calculus, but instead directly go
   by adapting the backward calculus of derived rules to the forward direction,
   and directly establishing soundness of this calculus with respect to the
   backward focused calculus.

** Focused inverse method
*** (frontier propositions)

* TODO Implementation details

  Implemented stuff:

  - Fully automatic size-bounded decision procedure for the logic presented here

  TODO
  - User queries (shape of the queries)
  - Size limit

  Other features... Like... curried functions as a monad.
  Or... search parameterized by a monad...

  Stuff...

* Futher work

  The following sections point out multiple directions towards which the work
  presented here could be extended. Most of them involve technical rather than
  conceptual difficulties in their realization, and they have been left out from
  the current implementation for lack of time more than anything.

** TODO Restoring the original Z-conditional

   In Section [TODO] we explained how the original Z-conditional of [paper]
   contains an implicit existential quantification: the introduction of a $A
   \rightarrow B$ connective witnesses the fact that /there exists some/
   transition from $A$ to $B$, with certain characteristics, whereas the
   elimination rule allows one to use a conditional $A \rightarrow B$ provided
   the surrounding context respects /all known instances/ of such connective.
   The development presented here takes a different approach, in the sense that
   it drops the implicit quantification and considers a conditional operator
   equipped with the information about the deduction that was used to establish
   it.

   As ..., the new conditional is more powerful than the one in [paper], an
   obvious consequence of the fact that it carries more information.  To restore
   the original Z-conditional, and thus the possibility of expressing formulas
   and proofs in the original language of Zsyntax, it is sufficient to restore
   what our conditional removed: the existential quantification. That is, the
   original Z-conditional can be obtained from ours by just existentially
   quantifying the data regarding elementary bases and reactions lists.  Our new
   conditional then becomes a predicate over terms of type ``elementary base''
   and ``reaction list'', where these terms can of course be arbitrary
   variables.

   \[
     A \rightarrow_Z B \sim \exists e l . A \rightarrow_l^e B
   \]

   Then, the original introduction rule becomes a derived rule:

   \[
     Gamma, A ... B     Gamma,Delta
     ------------------------------
	   A ->_l^e B, Delta
	   -----------------------
	   exists e l . ..., Delta
   \]

   Similarly, the original elimination rule can be derived as follows:

   \[
   Gamma, exists e l . A -> B       Gamma, A ->_l^e B ... Delta
   ------------------------------------------------------------
		   Gamma, Delta
   \]

   where $e$ and $l$ are fresh free variables that get discharged as part of the
   existential elimination rules.
   The real connection between the interpretation above and the original
   Z-conditional thus crucially depends on the interpretation of free variables.
   As usual, free variables represent a metalinguistic universal quantification,
   so that establishing the truth of $P(x)$ with $x$ free is to establish the
   truth of $P$ for an arbitrary $x$. In the same way, proving something from
   $P(x)$ requires us to prove it for any possible substitution of $x$.
   If the domain of $x$ is not fixed, this means that no assumption can be made
   on the nature of $x$.

   However, our logic and in particular the elimination rule of the
   Z-conditional in [paper], takes into account the current knowledge that we
   have about the domain at the moment we are doing a proof. This means that, in
   the case of $A \rightarrow_l^e B$ with $e$ and $l$ free, $e$ and $l$ should
   rather range over all concrete (not variables) terms that correspond to our
   current knowledge about transitions of the form $A \rightarrow B$.
   In other words, to prove something from $A \rightarrow_l^e B$ is to
   prove it for any possible substitution of $e$ and $l$ to non-variable
   elementary bases and reaction lists $\Delta$, $\mathrm{list}$ such that
   $A \rightarrow_{\mathrm{list}}^{\Delta} B$ is known to hold at the moment we
   are doing the proof.
   Under this treatment of existentials and free variables, it is easy to see
   that our interpretation corresponds to the original Z-conditional operator.

   The extension of the logic presented above with free variables and
   existential quantification is mainly a tiresome rather than conceptually
   challenging work, as it should only be an adaptation of [cmu thesis]. The
   only difficulty has to do with how to compute the domain over which free
   variables range during a certain proof. Consider a principal cut involving an
   existentially quantified conditional that is first introduced and then
   eliminated:

   \[
       Gamma |- A ->... B                  Delta, A ->e l B |- C
       ------------------------------     -------------------------------
       Gamma |- exists e l .A ->e l B     Delta, exists e l . A -> B |- C
       -------------------------------------------------------------------
			      ....
   \]

   To be able to move the cut to the premises, the premise on the right must
   have been derived with a domain of free variables that *includes* knowledge
   about the transition $A \rightarrow B$ that is proved in the left premise.
   This requires us to carefully assign an explicit temporal order
   interpretation to branches of a derivation, as the validity of parts of it
   may crucially depend on facts that are established in others that are
   intended to come before.
   This difficulties of course also arise when considering cuts of a
   Z-conditional in the original sense of [paper], since it stems from the
   existential quantification that is intrinsic in the connective. The only
   difference is that in our case the existential quantification is made
   explicit.

   At this point, we could ask ourselves if it would have been better to just
   consider the original Z-conditional and shape the calculus around its
   implicit existential quantification. Firstly, notice that this would not have
   resulted in a simpler logic. A logically correct and satisfactory
   implementation of the Z-conditional operator *is* difficult; this work just
   exposes why it is so, it does not introduce any difficulty in itself.

   Secondly, by hiding the quantification in the conditional, we would have
   removed useful expressive power from the logic.  A transition $A \rightarrow
   B$ given as an axiom is, for example, fundamentally different from the same
   transition obtained after a long proof involving several intermediate
   reactions. The ability to differentiate between such apperently identical
   operators also leads to new proofs, as advocated in section [todo].

   The approach proposed here is thus based on the observation that it is better
   to start with a more informative conditional operator (as the one presented
   here), and then *selectively* weaken (or generalize) it only when really
   needed and with very little cost, for example by quantifying the additional
   data on the conditional opearator to restore the semantics of the original
   Z-conditional. Conversely, it is much more expensive to try to recover
   information that we failed to express in the first place.

** TODO Remaining connectives

   The logic of Zsyntax was originally devised [2010 paper] with conjunction and
   implication connectives, and then extended [2015 paper] with a conjunctive
   unit and an additive disjunction operator to express internal choice, that is
   reactions that may evolve internally in multiple different ways without any
   action by the external observer.

   The logic presented here relates to the fragment of Zsyntax with
   multiplicative conjunction and implication only.
   These two connectives alone are already expressive enough to encode many
   useful situations, let alone all example use cases of Zsyntax syntax in the
   literature ([2010 paper], [melanoma paper]). Moreover, this fragment already
   includes the most interesting and powerful connective of Zsyntax, namely the
   Z-conditional, so it is worth studying by itself.

   We therefore choose to implement this fragment as it is sufficiently
   expressive to be useful in practice, sufficiently representative of the whole
   concept of Zsyntax and controlled monotonicity to represent a meaningful
   treatise of how an automated proof search procedure for Zsyntax could be
   implemented, and sufficiently small to be manageable in the small time
   available.

   Given that the main challenges regarding the implementation of Zsyntax are
   given by the conditional operator, we speculate that the extension of the
   present work to the remaining connectives, again following [cmu thesis] as a
   model, should be rather straightforward. This is because the additional
   connectives are not different from their linear logic counterparts in any
   fundamental way, so already existing literature on the subject can be
   leveraged.

** TODO Proof terms (meh...)

   The similarity between the calculus of Zsyntax and linear logic is made
   particularly clear by the sequent calculus presented here, where sequents and
   inference rules are just a suitably annotated counterpart of the ones for
   intuitionistic linear logic.
   This analogy suggests that it should be possible adapt the notion of proof
   terms for intuitionistic linear logic, formulated as a linear lambda
   calculus, to our case. Under this analogy, which would be just an instance of
   the Curry-Howard isomorphism, proof normalization would represent biological
   reaction.

** TODO Other
